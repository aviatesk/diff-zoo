var documenterSearchIndex = {"docs":
[{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"using Pkg\nPkg.activate(\"/home/runner/work/diff-zoo/diff-zoo/\")\nPkg.instantiate()\nfor f in [\"utils.jl\"]\n    cp(normpath(\"/home/runner/work/diff-zoo/diff-zoo/src\", f), normpath(@__DIR__, f), force = true)\nend","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"EditURL = \"https://github.com/aviatesk/diff-zoo/blob/master/src/forward.jl\"","category":"page"},{"location":"notebooks/forward/#Implementing-Forward-Mode","page":"Forward","title":"Implementing Forward Mode","text":"","category":"section"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"In the intro notebook we covered forward-mode differentiation thoroughly, but we don't yet have a real implementation that can work on our programs. Implementing AD effectively and efficiently is a field of its own, and we'll need to learn a few more tricks to get off the ground.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"include(\"utils.jl\");\nnothing #hide","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Up to now, we have differentiated things by creating a new Wengert list which contains parts of the original expression.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"y = Wengert(:(5sin(log(x))))\nderive(y, :x)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We're now going to explicitly split our lists into two pieces: the original expression, and a new one which only calculates derivatives (but might refer back to values from the first). For example:","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"y = Wengert(:(5sin(log(x))))","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"dy = derive(y, :x, out = Wengert(variable = :dy))","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Expr(dy)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"If we want to distinguish them, we can call y the primal code and dy the tangent code. Nothing fundamental has changed here, but it's useful to organise things this way.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Almost all of the subtlety in differentiating programs comes from a mathematically trivial question: in what order do we evaluate the statements of the Wengert list? We have discussed the forward/reverse distinction, but even once that choice is made, we have plenty of flexibility, and those choices can affect efficiency.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"For example, imagine if we straightforwardly evaluate y followed by dy. If we only cared about the final output of y, this would be no problem at all, but in general dy also needs to re-use variables like y1 (or possibly any y_i). If our primal Wengert list has, say, a billion instructions, we end up having to store a billion intermediate y_i before we run our tangent code.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Alternatively, one can imagine running each instruction of the tangent code as early as possible; as soon as we run y1 = log(x), for example, we know we can run dy2 = cos(y1) also. Then our final, combined program would look something like this:","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"y0 = x\ndy = 1\ny1 = log(y0)\ndy = dy/y0\ny2 = cos(y1)\ndy = dy*sin(y1)\n  ...","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Now we can throw out y1 soon after creating it, and we no longer have to store those billion intermediate results.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"The ability to do this is a very general property of forward differentiation; once we run a = f(b), we can then run fracdadx = fracdadb fracdbdx using only a and b. It's really just a case of replacing basic instructions like cos with versions that calculate both the primal and tangent at once.","category":"page"},{"location":"notebooks/forward/#Dual-Numbers","page":"Forward","title":"Dual Numbers","text":"","category":"section"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Finally, the trick that we've been building up to: making our programming language do this all for us! Almost all common languages – with the notable exception of C – provide good support for operator overloading, which allows us to do exactly this replacement.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"To start with, we'll make a container that holds both a y and a fracdydx, called a dual number.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"struct Dual{T<:Real} <: Real\n  x::T\n  ϵ::T\nend\n\nDual(1, 2)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Dual(1.0,2.0)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Let's print it nicely.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Base.show(io::IO, d::Dual) = print(io, d.x, \" + \", d.ϵ, \"ϵ\")\n\nDual(1, 2)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"And add some of our rules for differentiation. The rules have the same basic pattern-matching structure as the ones we originally applied to our Wengert list, just with different notation.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"import Base: +, -, *, /\na::Dual + b::Dual = Dual(a.x + b.x, a.ϵ + b.ϵ)\na::Dual - b::Dual = Dual(a.x - b.x, a.ϵ - b.ϵ)\na::Dual * b::Dual = Dual(a.x * b.x, b.x * a.ϵ + a.x * b.ϵ)\na::Dual / b::Dual = Dual(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ) / b.x^2)\n\nBase.sin(d::Dual) = Dual(sin(d.x), d.ϵ * cos(d.x))\nBase.cos(d::Dual) = Dual(cos(d.x), - d.ϵ * sin(d.x))\n\nDual(2, 2) * Dual(3, 4)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Finally, we'll hook into Julia's number promotion system; this isn't essential to understand, but just makes it easier to work with Duals since we can now freely mix them with other number types.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Base.convert(::Type{Dual{T}}, x::Dual) where T = Dual(convert(T, x.x), convert(T, x.ϵ))\nBase.convert(::Type{Dual{T}}, x::Real) where T = Dual(convert(T, x), zero(T))\nBase.promote_rule(::Type{Dual{T}}, ::Type{R}) where {T,R} = Dual{promote_type(T,R)}\n\nDual(1, 2) * 3","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We already have enough to start taking derivatives of some simple functions. If we pass a dual number into a function, the epsilon component represents the derivative.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"f(x) = x / (1 + x*x)\n\nf(Dual(5., 1.))","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We can make a utility which allows us to differentiate any function.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(f, x) = f(Dual(x, one(x))).ϵ\n\nD(f, 5.)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Dual numbers seem pretty scarcely related to all the Wengert list stuff we were talking about earlier. But we need take a closer look at how this is working. To start with, look at Julia's internal representation of f.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"@code_typed f(1.0)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"This is just a Wengert list! Though the naming is a little different – mul_float rather than the more general * and so on – it's still essentially the same data structure we were working with earlier. Moreover, you'll recognise the code for the derivative, too!","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"@code_typed D(f, 1.0)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"This code is again the same as the Wengert list derivative we worked out at the very beginning of this handbook. The order of operations is just a little different, and there's the odd missing or new instruction due to the different set of optimisations that Julia applies. Still, we have not escaped our fundamental symbolic differentiation algorithm, just tricked the compiler into doing most of the work for us.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"derive(Wengert(:(sin(cos(x)))), :x)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"@code_typed D(x -> sin(cos(x)), 0.5)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"What of data structures, control flow, function calls? Although these things are all present in Julia's internal \"Wengert list\", they end up being the same in the tangent program as in the primal; so an operator overloading approach need not deal with them explicitly to do the right thing. This won't be true when we come to talk more about reverse mode, which demands a more complex approach.","category":"page"},{"location":"notebooks/forward/#Perturbation-Confusion","page":"Forward","title":"Perturbation Confusion","text":"","category":"section"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Actually, that's not quite true. Operator-overloading-based forward mode almost always does the right thing, but it is not flawless. This more advanced section will talk about nested differentiation and the nasty bug that can come with it.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We can differentiate any function we want, as long as we have the right primitive definitions for it. For example, the derivative of sin(x) is cos(x).","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(sin, 0.5), cos(0.5)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We can also differentiate the differentiation operator itself. We'll find that the second derivative of sin(x) is -sin(x).","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(x -> D(sin, x), 0.5), -sin(0.5)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"This worked because we ended up nesting dual numbers. If we create a dual number whose epsilon component is another dual number, then we end up tracking the derivative of the derivative.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"The issue comes about when we close over a variable that is itself being differentiated.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(x -> x*D(y -> x+y, 1), 1) # == 1","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"The derivative fracddy (x + y) = 1, so this is equivalent to fracddxx, which should also be 1. So where did this go wrong? The problem is that when we closed over x, we didn't just get a numeric value but a dual number with epsilon = 1. When we then calculated x + y, both epsilons were added as if fracdxdy = 1 (effectively x = y). If we had written this down, the answer would be correct.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(x -> x*D(y -> y+y, 1), 1)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"I leave this second example as an excercise to the reader. Needless to say, this has caught out many an AD implementor.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"D(x -> x*D(y -> x*y, 1), 4) # == 8","category":"page"},{"location":"notebooks/forward/#More-on-Dual-Numbers","page":"Forward","title":"More on Dual Numbers","text":"","category":"section"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"The above discussion presented dual numbers as essentially being a trick for applying the chain rule. I wanted to take the opportunity to present an alternative viewpoint, which might be appealing if, like me, you have any training in physics.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Complex arithmetic involves a new number, i, which behaves like no other: specifically, because i^2 = -1. We'll introduce a number called epsilon, which is a bit like i except that epsilon^2 = 0; this is effectively a way of saying the epsilon is a very small number. The relevance of this comes from the original definition of differentiation, which also requires epsilon to be very small.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"fracddx f(x) = lim_epsilon to 0 fracf(x+epsilon)-f(x)epsilon","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"We can see how our definition of epsilon works out by applying it to f(x+epsilon); let's say that f(x) = sin(x^2).","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"beginaligned\nf(x + epsilon) = sin((x + epsilon)^2) \n                = sin(x^2 + 2xepsilon + epsilon^2) \n                = sin(x^2 + 2xepsilon) \n                = sin(x^2)cos(2xepsilon) + cos(x^2)sin(2xepsilon) \n                = sin(x^2) + 2xcos(x^2)epsilon \nendaligned","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"A few things have happened here. Firstly, we directly expand (x+epsilon)^2 and remove the epsilon^2 term. We expand sin(a+b) and then apply a small angle approximation: for small theta, sin(theta) approx theta and cos(theta) approx 1. (This sounds pretty hand-wavy, but does follow from our original definition of epsilon if we look at the Taylor expansion of both functions). Finally we can plug this into our derivative rule.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"beginaligned\nfracddx f(x) = fracf(x+epsilon)-f(x)epsilon \n                  = 2xcos(x^2)\nendaligned","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"This is, in my opinion, a rather nice way to derive functions by hand.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"This also leads to another nice trick, and a third way to look at forward-mode AD; if we replace x + epsilon with x + epsilon i then we still have (epsilon i)^2 = 0. If epsilon is a small real number (say 1times10^-10), this is still true within floating point error, so our derivative still works out.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"ϵ = 1e-10im\nx = 0.5\n\nf(x) = sin(x^2)\n\n(f(x+ϵ) - f(x)) / ϵ","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"2x*cos(x^2)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"So complex numbers can be used to get exact derivatives! This is very efficient and can be written using only one call to f.","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"imag(f(x+ϵ)) / imag(ϵ)","category":"page"},{"location":"notebooks/forward/","page":"Forward","title":"Forward","text":"Another way of looking at this is that we are doing bog-standard numerical differentiation, but the use of complex numbers avoids the typical problem with that technique (i.e. that a small perturbation ends up being overwhelmed by floating point error). The dual number is then a slight variation which makes the limit epsilon rightarrow 0 exact, rather than approximate. Forward mode AD can be described as \"just\" a clever implementation of numerical differentiation. Both numerical and forward derivatives propagate a perturbation epsilon using the same basic rules, and they have the same algorithmic properties.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"using Pkg\nPkg.activate(\"/home/runner/work/diff-zoo/diff-zoo/\")\nPkg.instantiate()\nfor f in [\"utils.jl\"]\n    cp(normpath(\"/home/runner/work/diff-zoo/diff-zoo/src\", f), normpath(@__DIR__, f), force = true)\nend","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"EditURL = \"https://github.com/aviatesk/diff-zoo/blob/master/src/backandforth.jl\"","category":"page"},{"location":"notebooks/backandforth/#Forward-and-Reverse-Mode-Differentiation","page":"Back & Forth","title":"Forward- and Reverse-Mode Differentiation","text":"","category":"section"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"include(\"utils.jl\");\nnothing #hide","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Differentiation tools are frequently described as implementing \"forward mode\" or \"reverse mode\" AD. This distinction was briefly covered in the intro notebook, but here we'll go into more detail. We'll start with an intuition for what the distinction means in terms of the differentiation process; then we'll discuss why it's an important consideration in practice.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Consider a simple mathematical expression:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"y = :(sin(x^2) * 5)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Written as a Wengert list:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Wengert(y)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"The ability to take derivatives mechanically relies on two things: Firstly, we know derivatives for each basic function in our program (e.g. fracdy_2dy_1=cos(y_1)). Secondly, we have a rule of composition called the chain rule which lets us compose these basic derivatives together.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"fracdydx = fracdy_1dx times fracdy_2dy_1 times fracdydy_2","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"More specifically:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"beginaligned\nfracdydx = 2x times cos(y_1) times 5 \n              = 2x times cos(x^2) times 5\nendaligned","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"The forward/reverse distinction basically amounts to: given that we do multiplications one at a time, do we evaluate fracdy_1dx times fracdy_2dy_1 first, or fracdy_2dy_1 times fracdydy_2? (This seems like a pointless question right now, given that either gets us the same results, but bear with me.)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"It's easier to see the distinction if we think algorithmically. Given some enormous Wengert list with n instructions, we have two ways to differentiate it:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"(1): Start with the known quantity fracdy_0dx = fracdxdx = 1 at the beginning of the list. Look up the derivative for the next instruction fracdy_i+1dy_i and multiply out the top, getting fracdy_1dx, fracdy_2dx, ... fracdy_n-1dx, fracdydx. Because we walked forward over the Wengert list, this is called forward mode. Each intermediate derivative fracdy_idx is known as a perturbation.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"(2): Start with the known quantity fracdydy_n = fracdydy = 1 at the end of the list. Look up the derivative for the previous instruction fracdy_idy_i-1 and multiply out the bottom, getting fracdydy_n, fracdydy_n-1, ... fracdydy_1, fracdydx. Because we walked in reverse over the Wengert list, this is called reverse mode. Each intermediate derivative fracdydy_i is known as a sensitivity.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"This all seems very academic, so we need to explain why it might make a difference to performance. I'll give two related explanations: dealing with mulitple variables, and working with vectors rather than scalars.","category":"page"},{"location":"notebooks/backandforth/#Explanation-1:-Multiple-Variables","page":"Back & Forth","title":"Explanation 1: Multiple Variables","text":"","category":"section"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"So far we have dealt only with simple functions that take a number, and return a number. But more generally we'll deal with functions that take, or produce, multiple numbers of interest.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"For example what if we have a function that returns two numbers, and we want derivatives for both? Do we have to do the whole thing twice over?","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"y = quote\n  y2 = sin(x^2)\n  y3 = y2 * 5\nend","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Let's say we want both of the derivatives fracdy_2dx and fracdy_3dx. You can probably see where this is going now; the Wengert list representation of this expression has not actually changed!","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Wengert(y)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Now, we discussed that when doing forward mode differentiation, we actually calculate every intermediate derivative fracdy_idx, which means we get fracdy_2dx for free. This property goes all the way back to our original, recursive formulation of differentiation, which calculated the derivatives of a complex expression by combining the derivatives of simpler ones.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"derive(Wengert(y), :x)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"In our output, y_7 = fracdy_2dx and y_8 = fracdy_3dx.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Let's consider the opposite situation, a function of two variables a and b, where we'd like to get fracdyda and fracdydb.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"y = :(sin(a) * b)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Wengert(y)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"This one is a bit tougher. We can start the forward-mode differentiation process with fracdada = 1 or with fracdbdb = 1, but if we want both we'll have to go through the entire multiplying-out process twice.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"But both variables ultimately end up at the same place, y, and we know that fracdydy = 1. Aha, so perhaps we can use reverse mode for this instead!","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"Exactly opposite to forward mode, reverse mode gives us every intermediate gradient fracdy_idy for free, ultimately leading back in the inputs fracdady and fracdbdy.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"It's easy to see, then, why reverse-mode differentiation – or backpropagation – is so effective for machine learning. In general we have a large computation with millions of parameters, yet only a single scalar loss to optimise. We can get gradients even for these millions of inputs in a single pass, enabling ML to scale to complex tasks like image and voice recognition.","category":"page"},{"location":"notebooks/backandforth/#Explanation-2:-Vector-Calculus","page":"Back & Forth","title":"Explanation 2: Vector Calculus","text":"","category":"section"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"So far we have dealt only with simple functions that take a number, and return a number. But more generally we'll deal with functions that take, or produce, vectors containing multiple numbers of interest.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"It's useful to consider how our idea of differentiation works when we have vectors. For example, a function that takes a vector of length 2 to another vector of length 2:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"f(x) = [x[1] * x[2], cos(x[1])]\n\nx = [2, 3]\ny = f(x)","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"We now need to talk about what we mean by fracddxf(x), given that we can't apply the usual limit rule. What we can do is take the derivative of any scalar element of y with respect to any element of x. (We'll use subscripts x_n to refer to the n^th index of x.) For example:","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"beginaligned\nfracdy_1dx_1 = fracddx_1 x_1 times x_2 = x_2 \nfracdy_1dx_2 = fracddx_2 x_1 times x_2 = x_1 \nfracdy_2dx_1 = fracddx_1 cos(x_1) = -sin(x_1) \nfracdy_2dx_2 = fracddx_2 cos(x_1) = 0 \nendaligned","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"It's a little easier if we organise all of these derivatives into a matrix.","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"J_ij = fracdy_idx_j","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"This 2times2 matrix is called the Jacobian, and in general it's what we mean by fracdydx. (The Jacobian for a scalar function like y = sin(x) only has one element, so it's consistent with our current idea of the derivative fracdydx.) The key point here is that the Jacobian is a potentially large object: it has a size length(y) * length(x). Now, we discussed that the distinction between forward and reverse mode is whether we propagate fracdy_idx or fracdydy_i, which can have a size of either length(y_i) * length(x) or length(y) * length(y_i).","category":"page"},{"location":"notebooks/backandforth/","page":"Back & Forth","title":"Back & Forth","text":"It should be clear, then, what mode is better if we have a gazillion inputs and one output. In forward mode we need to carry around a gazillion \"perturbations\" for each element of y_i, whereas in reverse we only need a gradient of the same size of x. And vice versa.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"using Pkg\nPkg.activate(\"/home/runner/work/diff-zoo/diff-zoo/\")\nPkg.instantiate()\nfor f in [\"utils.jl\"]\n    cp(normpath(\"/home/runner/work/diff-zoo/diff-zoo/src\", f), normpath(@__DIR__, f), force = true)\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"EditURL = \"https://github.com/aviatesk/diff-zoo/blob/master/src/intro.jl\"","category":"page"},{"location":"notebooks/intro/#Differentiation-for-Hackers","page":"Intro","title":"Differentiation for Hackers","text":"","category":"section"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"These notebooks are an exploration of various approaches to analytical differentiation. Differentiation is something you learned in school; we start with an expression like y = 3x^2 + 2x + 1 and find an expression for the derivative like fracdydx = 6x + 2. Once we have such an expression, we can evaluate it by plugging in a specific value for x (say 0.5) to find the derivative at that point (in this case fracdydx = 5).","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Despite its surface simplicity, this technique lies at the core of all modern machine learning and deep learning, alongside many other parts of statistics, mathematical optimisation and engineering. There has recently been an explosion in automatic differentiation (AD) tools, all with different designs and tradeoffs, and it can be difficult to understand how they relate to each other.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"We aim to fix this by beginning with the \"calculus 101\" rules that you are familiar with and implementing simple symbolic differentiators over mathematical expressions. Then we show how tweaks to this basic framework generalise from expressions to programming languages, leading us to modern automatic differentiation tools and machine learning frameworks like TensorFlow and PyTorch, and giving us a unified view across the AD landscape.","category":"page"},{"location":"notebooks/intro/#Symbolic-Differentiation","page":"Intro","title":"Symbolic Differentiation","text":"","category":"section"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"To talk about derivatives, we need to talk about expressions, which are symbolic forms like x^2 + 1 (as opposed to numbers like 5). Normal Julia programs only work with numbers; we can write down x^2 + 1 but this only lets us calculate its value for a specific x.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"x = 2\ny = x^2 + 1","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"However, Julia also offers a quotation operator which lets us talk about the expression itself, without needing to know what x is.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(x^2 + 1)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"typeof(y)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Expressions are a tree data structure. They have a head which tells us what kind of expression they are (say, a function call or if statement). They have args, their children, which may be further sub-expressions. For example, x^2 + 1 is a call to +, and one of its children is the expression x^2.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y.head","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y.args","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"We could have built this expression by hand rather than using quotation. It's just a bog-standard tree data structure that happens to have nice printing.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"x2 = Expr(:call, :^, :x, 2)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = Expr(:call, :+, x2, 1)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"We can evaluate our expression to get a number out.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"eval(y)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"When we differentiate something, we'll start by manipulating an expression like this, and then we can optionally evaluate it with numbers to get a numerical derivative. I'll call these the \"symbolic phase\" and the \"numeric phase\" of differentiation, respectively.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"How might we differentiate an expression like x^2 + 1? We can start by looking at the basic rules in differential calculus.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"beginaligned\nfracddx x = 1 \nfracddx (-u) = - fracdudx \nfracddx (u + v) = fracdudx + fracdvdx \nfracddx (u * v) = v fracdudx + u fracdvdx \nfracddx (u  v) = (v fracdudx - u fracdvdx)  v^2 \nfracddx u^n = n u^n-1 \nendaligned","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Seeing fracddx(u) as a function, these rules look a lot like a recursive algorithm. To differentiate something like y = a + b, we differentiate a and b and combine them together. To differentiate a we do the same thing, and so on; eventually we'll hit something like x or 3 which has a trivial derivative (1 or 0).","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Let's start by handling the obvious cases, y = x and y = 1.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive(ex, x)\n  ex == x ? 1 :\n  ex isa Union{Number,Symbol} ? 0 :\n  error(\"$ex is not differentiable\")\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(x)\nderive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(1)\nderive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"We can look for expressions of the form y = a + b using pattern matching, with a package called MacroTools. If @capture returns true, then we can work with the sub-expressions a and b.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"using MacroTools\n\ny = :(x + 1)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"@capture(y, a_ * b_)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"@capture(y, a_ + b_)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"a, b","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Let's use this to add a rule to derive, following the chain rule above.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive(ex, x)\n  ex == x ? 1 :\n  ex isa Union{Number,Symbol} ? 0 :\n  @capture(ex, a_ + b_) ? :($(derive(a, x)) + $(derive(b, x))) :\n  error(\"$ex is not differentiable\")\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(x + 1)\nderive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(x + (1 + (x + 1)))\nderive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"These are the correct derivatives, even if they could be simplified a bit. We can go on to add the rest of the rules similarly.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive(ex, x)\n  ex == x ? 1 :\n  ex isa Union{Number,Symbol} ? 0 :\n  @capture(ex, a_ + b_) ? :($(derive(a, x)) + $(derive(b, x))) :\n  @capture(ex, a_ * b_) ? :($a * $(derive(b, x)) + $b * $(derive(a, x))) :\n  @capture(ex, a_^n_Number) ? :($(derive(a, x)) * ($n * $a^$(n-1))) :\n  @capture(ex, a_ / b_) ? :($b * $(derive(a, x)) - $a * $(derive(b, x)) / $b^2) :\n  error(\"$ex is not differentiable\")\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This is enough to get us a slightly more difficult derivative.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(3x^2 + (2x + 1))\ndy = derive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This is correct – it's equivalent to 6x + 2 – but it's also a bit noisy, with a lot of redundant terms like x + 0. We can clean this up by creating some smarter functions to do our symbolic addition and multiplication. They'll just avoid actually doing anything if the input is redundant.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"addm(a, b) = a == 0 ? b : b == 0 ? a : :($a + $b)\nmulm(a, b) = 0 in (a, b) ? 0 : a == 1 ? b : b == 1 ? a : :($a * $b)\nmulm(a, b, c...) = mulm(mulm(a, b), c...)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"addm(:a, :b)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"addm(:a, 0)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"mulm(:b, 1)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Our tweaked derive function:","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive(ex, x)\n  ex == x ? 1 :\n  ex isa Union{Number,Symbol} ? 0 :\n  @capture(ex, a_ + b_) ? addm(derive(a, x), derive(b, x)) :\n  @capture(ex, a_ * b_) ? addm(mulm(a, derive(b, x)), mulm(b, derive(a, x))) :\n  @capture(ex, a_^n_Number) ? mulm(derive(a, x),n,:($a^$(n-1))) :\n  @capture(ex, a_ / b_) ? :($(mulm(b, derive(a, x))) - $(mulm(a, derive(b, x))) / $b^2) :\n  error(\"$ex is not differentiable\")\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"And the output is much cleaner.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(3x^2 + (2x + 1))\ndy = derive(y, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Having done this, we can also calculate a nested derivative fracd^2ydx^2, and so on.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"ddy = derive(dy, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(ddy, :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"There is a deeper problem with our differentiation algorithm, though. Look at how big this derivative is.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2)), :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Adding an extra * x makes it even bigger! There's a bunch of redundant work here, repeating the expression 1 + x^2 three times over.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2) * x), :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This happens because our rules look like, fracd(u*v)dx = u*fracdvdx + v*fracdudx. Every multiplication repeats the whole sub-expression and its derivative, making the output exponentially large in the size of its input.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This seems to be an achilles heel for our little differentiator, since it will make it impractical to run on any realistically-sized program. But wait! Things are not quite as simple as they seem, because this expression is not actually as big as it looks.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Imagine we write down:","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y1 = :(1 * 2)\ny2 = :($y1 + $y1 + $y1 + $y1)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This looks like a large expression, but in actual fact it does not contain 1*2 four times over, just four pointers to y1; it is not really a tree but a graph that gets printed as a tree. We can show this by explicitly printing the expression in a way that preserves structure.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"(The definition of printstructure is not important to understand, but is here for reference.)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"printstructure(x, _, _) = x\n\nfunction printstructure(ex::Expr, cache = IdDict(), n = Ref(0))\n  haskey(cache, ex) && return cache[ex]\n  args = map(x -> printstructure(x, cache, n), ex.args)\n  cache[ex] = sym = Symbol(:y, n[] += 1)\n  println(:($sym = $(Expr(ex.head, args...))))\n  return sym\nend\n\nprintstructure(y2);\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Note that this is not the same as running common subexpression elimination to simplify the tree, which would have an O(n^2) computational cost. If there is real duplication in the expression, it'll show up.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":":(1*2 + 1*2) |> printstructure;\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"This is effectively a change in notation: we were previously using a kind of \"calculator notation\" in which any computation used more than once had to be repeated in full. Now we are allowed to use variable bindings to get the same effect.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"If we try printstructure on our differentiated code, we'll see that the output is not so bad after all:","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":":(x / (1 + x^2)) |> printstructure;\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2)), :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2)), :x) |> printstructure;\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"The expression x^2 + 1 is now defined once and reused rather than being repeated, and adding the extra * x now adds a couple of instructions to our derivative, rather than doubling its size. It turns out that our \"naive\" symbolic differentiator actually preserves structure in a very sensible way, and we just needed the right program representation to exploit that.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2) * x), :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2) * x), :x) |> printstructure;\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Calculator notation – expressions without variable bindings – is a terrible format for anything, and will tend to blow up in size whether you differentiate it or not. Symbolic differentiation is commonly criticised for its susceptability to \"expression swell\", but in fact has nothing to do with the differentiation algorithm itself, and we need not change it to get better results.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Conversely, the way we have used Expr objects to represent variable bindings is perfectly sound, if a little unusual. This format could happily be used to illustrate all of the concepts in this handbook, and the recursive algorithms used to do so are elegant. However, it will clarify some things if they are written a little more explicitly; for this we'll introduce a new, equivalent representation for expressions.","category":"page"},{"location":"notebooks/intro/#The-Wengert-List","page":"Intro","title":"The Wengert List","text":"","category":"section"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"The output of printstructure above is known as a \"Wengert List\", an explicit list of instructions that's a bit like writing assembly code. Really, Wengert lists are nothing more or less than mathematical expressions written out verbosely, and we can easily convert to and from equivalent Expr objects.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"include(\"utils.jl\");\nnothing #hide","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"y = :(3x^2 + (2x + 1))","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"wy = Wengert(y)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Expr(wy)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Inside, we can see that it really is just a list of function calls, where y_n refers to the result of the n^th.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"wy.instructions","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Like Exprs, we can also build Wengert lists by hand.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"w = Wengert()\ntmp = push!(w, :(x^2))\nw","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"push!(w, :($tmp + 1))\nw","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"Armed with this, we can quite straightforwardly port our recursive symbolic differentiation algorithm to the Wengert list.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive(ex, x, w)\n  ex isa Variable && (ex = w[ex])\n  ex == x ? 1 :\n  ex isa Union{Number,Symbol} ? 0 :\n  @capture(ex, a_ + b_) ? push!(w, addm(derive(a, x, w), derive(b, x, w))) :\n  @capture(ex, a_ * b_) ? push!(w, addm(mulm(a, derive(b, x, w)), mulm(b, derive(a, x, w)))) :\n  @capture(ex, a_^n_Number) ? push!(w, mulm(derive(a, x, w),n,:($a^$(n-1)))) :\n  @capture(ex, a_ / b_) ? push!(w, :($(mulm(b, derive(a, x, w))) - $(mulm(a, derive(b, x, w))) / $b^2)) :\n  error(\"$ex is not differentiable\")\nend\n\nderive(w::Wengert, x) = (derive(w[end], x, w); w)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"It behaves identically to what we wrote before; we have only changed the underlying representation.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(Wengert(:(3x^2 + (2x + 1))), :x) |> Expr","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"In fact, we can compare them directly using the printstructure function we wrote earlier.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(:(x / (1 + x^2)), :x) |> printstructure","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(Wengert(:(x / (1 + x^2))), :x)","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"They are almost identical; the only difference is the unused variable y3 in the Wengert version. This happens because our Expr format effectively removes dead code for us automatically. We'll see the same thing happen if we convert the Wengert list back into an Expr.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive(Wengert(:(x / (1 + x^2))), :x) |> Expr\n\nfunction derive(w::Wengert, x)\n  ds = Dict()\n  ds[x] = 1\n  d(x) = get(ds, x, 0)\n  for v in keys(w)\n    ex = w[v]\n    Δ = @capture(ex, a_ + b_) ? addm(d(a), d(b)) :\n        @capture(ex, a_ * b_) ? addm(mulm(a, d(b)), mulm(b, d(a))) :\n        @capture(ex, a_^n_Number) ? mulm(d(a),n,:($a^$(n-1))) :\n        @capture(ex, a_ / b_) ? :($(mulm(b, d(a))) - $(mulm(a, d(b))) / $b^2) :\n        error(\"$ex is not differentiable\")\n    ds[v] = push!(w, Δ)\n  end\n  return w\nend\n\nderive(Wengert(:(x / (1 + x^2))), :x) |> Expr","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"One more thing. The astute reader may notice that our differentiation algorithm begins with fracdxdx=1 and propagates this forward to the output; in other words it does forward-mode differentiation. We can tweak our code a little to do reverse mode instead.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"function derive_r(w::Wengert, x)\n  ds = Dict()\n  d(x) = get(ds, x, 0)\n  d(x, Δ) = ds[x] = haskey(ds, x) ? addm(ds[x],Δ) : Δ\n  d(lastindex(w), 1)\n  for v in reverse(collect(keys(w)))\n    ex = w[v]\n    Δ = d(v)\n    if @capture(ex, a_ + b_)\n      d(a, Δ)\n      d(b, Δ)\n    elseif @capture(ex, a_ * b_)\n      d(a, push!(w, mulm(Δ, b)))\n      d(b, push!(w, mulm(Δ, a)))\n    elseif @capture(ex, a_^n_Number)\n      d(a, mulm(Δ, n, :($a^$(n-1))))\n    elseif @capture(ex, a_ / b_)\n      d(a, push!(w, mulm(Δ, b)))\n      d(b, push!(w, :(-$(mulm(Δ, a))/$b^2)))\n    else\n      error(\"$ex is not differentiable\")\n    end\n  end\n  push!(w, d(x))\n  return w\nend","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"There are only two distinct algorithms in this handbook, and this is the second! It's quite similar to forward mode, with the difference that we walk backwards over the list, and each time we see a usage of a variable y_i we accumulate a gradient for that variable.","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"derive_r(Wengert(:(x / (1 + x^2))), :x) |> Expr","category":"page"},{"location":"notebooks/intro/","page":"Intro","title":"Intro","text":"For now, the output looks pretty similar to that of forward mode; we'll explain why the distinction makes a difference in future notebooks.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"using Pkg\nPkg.activate(\"/home/runner/work/diff-zoo/diff-zoo/\")\nPkg.instantiate()\nfor f in [\"utils.jl\"]\n    cp(normpath(\"/home/runner/work/diff-zoo/diff-zoo/src\", f), normpath(@__DIR__, f), force = true)\nend","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"EditURL = \"https://github.com/aviatesk/diff-zoo/blob/master/src/tracing.jl\"","category":"page"},{"location":"notebooks/tracing/#Tracing-based-Automatic-Differentiation","page":"Tracing","title":"Tracing-based Automatic Differentiation","text":"","category":"section"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Machine learning primarily needs reverse-mode AD, and tracing / operator overloading approaches are by far the most popular way to it; this is the technique used by ML frameworks from Theano to PyTorch. This notebook will cover the techniques used by those frameworks, as well as clarifying the distinction between the \"static declaration\" (Theano/TensorFlow) and \"eager execution\" (Chainer/PyTorch/Flux) approaches to AD.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"include(\"utils.jl\")","category":"page"},{"location":"notebooks/tracing/#Partial-Evaluation","page":"Tracing","title":"Partial Evaluation","text":"","category":"section"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Say we have a simple implementation of x^n which we want to differentiate.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"function pow(x, n)\n  r = 1\n  for i = 1:n\n    r *= x\n  end\n  return r\nend\n\npow(2, 3)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"We already know how to differentiate Wengert lists, but this doesn't look much like one of those. In fact, we can't write this program as a Wengert list at all, given that it contains control flow; and more generally our programs might have things like data structures or function calls that we don't know how to differentiate either.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Though it's possible to generalise the Wengert list to handle these things, there's actually a simple and surprisingly effective alternative, called \"partial evaluation\". This means running some part of a program without running all of it. For example, given an expression like x + 5 * n where we know n = 3, we can simplify to x + 15 even though we don't know what x is. This is a common trick in compilers, and Julia will often do it for you:","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"f(x, n) = x + 5 * n\ng(x) = f(x, 3)\n\ncode_typed(g, Tuple{Int})[1]","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"This suggests a solution to our dilemma above. If we know what n is (say, 3), we can write pow(x, 3) as ((1*x)*x)*x, which is a Wengert expression that we can differentiate. In effect, this is a kind of compilation from a complex language (Julia, Python) to a much simpler one.","category":"page"},{"location":"notebooks/tracing/#Static-Declaration","page":"Tracing","title":"Static Declaration","text":"","category":"section"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"We want to trace all of the basic mathematical operations in the program, stripping away everything else. We'll do this using Julia's operator overloading; the idea is to create a new type which, rather than actually executing operations like a + b, records them into a Wengert list.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"import Base: +, -\n\nstruct Staged\n  w::Wengert\n  var\nend\n\na::Staged + b::Staged = Staged(w, push!(a.w, :($(a.var) + $(b.var))))\n\na::Staged - b::Staged = Staged(w, push!(a.w, :($(a.var) - $(b.var))))","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Actually, all of our staged definitions follow the same pattern, so we can just do them in a loop. We also add an extra method so that we can multiply staged values by constants.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"for f in [:+, :*, :-, :^, :/]\n  @eval Base.$f(a::Staged, b::Staged) = Staged(a.w, push!(a.w, Expr(:call, $(Expr(:quote, f)), a.var, b.var)))\n  @eval Base.$f(a, b::Staged) = Staged(b.w, push!(b.w, Expr(:call, $(Expr(:quote, f)), a, b.var)))\nend","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"The idea here is to begin by creating a Wengert list (the \"graph\" in ML framework parlance), and create some symbolic variables which do not yet have numerical values.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"w = Wengert()\nx = Staged(w, :x)\ny = Staged(w, :y)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"When we manipulate these variables, we'll get Wengert lists.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"z = 2x + y\nz.w |> Expr","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Crucially, this works with our original pow function!","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"w = Wengert()\nx = Staged(w, :x)\n\ny = pow(x, 3)\ny.w |> Expr","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"The rest is almost too easy! We already know how to derive this.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"dy = derive_r(y.w, :x)\nExpr(dy)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"If we dump the derived code into a function, we get code for the derivative of x^3 at any point (i.e. 3x^2).","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"@eval dcube(x) = $(Expr(dy))\n\ndcube(5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Congratulations, you just implemented TensorFlow.","category":"page"},{"location":"notebooks/tracing/#Eager-Execution","page":"Tracing","title":"Eager Execution","text":"","category":"section"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"This approach has a crucial problem; because it works by stripping out control flow and parameters like n, it effectively freezes all of these things. We can get a specific derivative for x^3, x^4 and so on, but we can't get the general derivative of x^n with a single Wengert list. This puts a severe limitation on the kinds of models we can express.[1]","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"The solution? Well, just re-build the Wengert list from scratch every time!","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"function D(f, x)\n  x_ = Staged(w, :x)\n  dy = derive(f(x_).w, :x)\n  eval(:(let x = $x; $(Expr(dy)) end))\nend\n\nD(x -> pow(x, 3), 5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"D(x -> pow(x, 5), 5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"This gets us our gradients, but it's not going to be fast – there's a lot of overhead to building and evaluating the list/graph every time. There are two things we can do to alleviate this:","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Interpret, rather compile, the Wengert list.\nFuse interpretation of the list (the numeric phase) with the building and manipulation of the Wengert list (the symbolic phase).","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Implementing this looks a lot like the Staged object above. The key difference is that alongside the Wengert list, we store the numerical values of each variable and instruction as we go along. Also, rather than explicitly naming variables x, y etc, we generate names using gensym().","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"gensym()","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"struct Tape\n  instructions::Wengert\n  values\nend\n\nTape() = Tape(Wengert(), Dict())\n\nstruct Tracked\n  w::Tape\n  var\nend\n\nfunction track(t::Tape, x)\n  var = gensym()\n  t.values[var] = x\n  Tracked(t, var)\nend\n\nBase.getindex(x::Tracked) = x.w.values[x.var]\n\nfor f in [:+, :*, :-, :^, :/]\n  @eval function Base.$f(a::Tracked, b::Tracked)\n    var = push!(a.w.instructions, Expr(:call, $(Expr(:quote, f)), a.var, b.var))\n    a.w.values[var] = $f(a[], b[])\n    Tracked(a.w, var)\n  end\n  @eval function Base.$f(a, b::Tracked)\n    var = push!(b.w.instructions, Expr(:call, $(Expr(:quote, f)), a, b.var))\n    b.w.values[var] = $f(a, b[])\n    Tracked(b.w, var)\n  end\n  @eval function Base.$f(a::Tracked, b)\n    var = push!(a.w.instructions, Expr(:call, $(Expr(:quote, f)), a.var, b))\n    a.w.values[var] = $f(a[], b)\n    Tracked(a.w, var)\n  end\nend","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Now, when we call pow it looks a lot more like we are dealing with normal numeric values; but there is still a Wengert list inside.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"t = Tape()\nx = track(t, 5)\n\ny = pow(x, 3)\ny[]\n\ny.w.instructions |> Expr","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Finally, we need to alter how we derive this list. The key insight is that since we already have values available, we don't need to explicitly build and evaluate the derivative code; instead, we can just evaluate each instruction numerically as we go along. We more-or-less just need to replace our symbolic functions like (addm) with the regular ones (+).","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"This is, of course, not a particularly optimised implementation, and faster versions have many more tricks up their sleaves. But this gets at all the key ideas.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"function derive(w::Tape, xs...)\n  ds = Dict()\n  val(x) = get(w.values, x, x)\n  d(x) = get(ds, x, 0)\n  d(x, Δ) = ds[x] = d(x) + Δ\n  d(lastindex(w.instructions), 1)\n  for v in reverse(collect(keys(w.instructions)))\n    ex = w.instructions[v]\n    Δ = d(v)\n    if @capture(ex, a_ + b_)\n      d(a, Δ)\n      d(b, Δ)\n    elseif @capture(ex, a_ * b_)\n      d(a, Δ * val(b))\n      d(b, Δ * val(a))\n    elseif @capture(ex, a_^n_Number)\n      d(a, Δ * n * val(a) ^ (n-1))\n    elseif @capture(ex, a_ / b_)\n      d(a, Δ * val(b))\n      d(b, -Δ*val(a)/val(b)^2)\n    else\n      error(\"$ex is not differentiable\")\n    end\n  end\n  return map(x -> d(x.var), xs)\nend\n\nderive(y.w, x)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"With this we can implement a more general gradient function.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"function gradient(f, xs...)\n  t = Tape()\n  xs = map(x -> track(t, x), xs)\n  f(xs...)\n  derive(t, xs...)\nend","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Even with the limited set of gradients that we have, we're well on our way to differentiating more complex programs, like a custom sin function.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"gradient((a, b) -> a*b, 2, 3)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"mysin(x) = sum((-1)^k/factorial(1.0+2k) * x^(1+2k) for k = 0:5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"gradient(mysin, 0.5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"cos(0.5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"We can even take nested derivatives!","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"gradient(x -> gradient(mysin, x)[1], 0.5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"-sin(0.5)","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Though the tracing approach has significant limitations, its power is in how easy it is to implement: one can build a fairly full-featured implementation, in almost any language, in a weekend. Almost all languages have the operator-overloading features required, and no matter how complex the host language, one ends up with a simple Wengert list.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Note that we have not removed the need to apply our basic symbolic differentiation algorithm here. We are still looking up gradient definitions, reversing data flow and applying the chain rule – it's just interleaved with our numerical operations, and we avoid building the output into an explicit Wengert list.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"It's somewhat unusual to emphasise the symbolic side of AD, but I think it gives us an incisive way to understand the tradeoffs that different systems make. For example: TensorFlow-style AD has its numeric phase separate from Python's runtime, which makes it awkward to use. Conversely, PyTorch does run its numerical phase at runtime, but also its symbolic phase, making it impossible to optimise the backwards pass.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"We observed that OO-based forward mode is particularly successful because it carries out its symbolic and numeric operations at Julia's compile and run time, respectively. In the source to source reverse mode notebook, we'll explore doing this for reverse mode as well.","category":"page"},{"location":"notebooks/tracing/#Footnotes","page":"Tracing","title":"Footnotes","text":"","category":"section"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"[1]: Systems like TensorFlow can also just provide ways to inject control flow  into the graph. This brings us closer to a source-to-source  approach where Python is used to build an expression in  TensorFlows internal graph language.","category":"page"},{"location":"notebooks/tracing/","page":"Tracing","title":"Tracing","text":"Fun fact: PyTorch and Flux's tapes are actually closer to the Expr format that we originally used, in which \"tracked\" tensors just have pointers to their parents (implicitly forming a graph/Wengert list/expression tree). A naive algorithm for backpropagation suffers from exponential runtime for the exact same reason that naive symbolic diff does; \"flattening\" this graph into a tree causes it to blow up in size.","category":"page"},{"location":"","page":"README","title":"README","text":"note: Note\nThis is a clone of the \"Differentiation for Hackers\" handbook, written by Mike J. Innes. All the purpose of this clone is to render the notebooks with Documenter.jl and host them on GitHub Pages. The notebooks rendered by Documenter.jl are available here. Except that, all the contents should be identical, and all the credit goes to him.","category":"page"},{"location":"#Differentiation-for-Hackers","page":"README","title":"Differentiation for Hackers","text":"","category":"section"},{"location":"","page":"README","title":"README","text":"(Image: Build Status) (Image: )","category":"page"},{"location":"","page":"README","title":"README","text":"The goal of this handbook is to demystify algorithmic differentiation, the tool that underlies modern machine learning. It begins with a calculus-101 style understanding and gradually extends this to build toy implementations of systems similar to PyTorch and TensorFlow. I have tried to clarify the relationships between every kind of differentiation I can think of – including forward and reverse, symbolic, numeric, tracing and source transformation. Where typical real-word ADs are mired in implementation details, these implementations are designed to be coherent enough that the real, fundamental differences – of which there are surprisingly few – become obvious.","category":"page"},{"location":"","page":"README","title":"README","text":"The intro notebook is recommended to start with, but otherwise notebooks do not have a fixed order.","category":"page"},{"location":"","page":"README","title":"README","text":"Intro – explains the basics, beginning with a simple symbolic differentiation routine.\nBack & Forth – discusses the difference between forward and reverse mode AD.\nForward – discusses forward-mode AD and its relationship to symbolic and numerical differentiation.\nTracing – discusses tracing-based implementations of reverse mode, as used by TensorFlow and PyTorch.\nReverse – discusses a more powerful reverse mode based on source transformation (not complete).","category":"page"},{"location":"","page":"README","title":"README","text":"If you want to run the notebooks locally, they can be built by running the src/notebooks.jl script using Julia. They should appear inside a /notebooks folder. Alternatively, you can run through the scripts in Juno.","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"using Pkg\nPkg.activate(\"/home/runner/work/diff-zoo/diff-zoo/\")\nPkg.instantiate()\nfor f in [\"utils.jl\"]\n    cp(normpath(\"/home/runner/work/diff-zoo/diff-zoo/src\", f), normpath(@__DIR__, f), force = true)\nend","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"EditURL = \"https://github.com/aviatesk/diff-zoo/blob/master/src/reverse.jl\"","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"include(\"utils.jl\");\nnothing #hide","category":"page"},{"location":"notebooks/reverse/#Source-to-Source-Reverse-Mode","page":"Reverse","title":"Source to Source Reverse Mode","text":"","category":"section"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"Forward mode works well because all of the symbolic operations happen at Julia's compile time; Julia can then optimise the resulting program (say, by applying SIMD instructions) and we get very fast derivative code. Although we can differentiate Julia code by compiling it to a Wengert list, we'd be much better off if we could handle Julia code directly; then reverse mode can benefit from these optimisations too.","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"However, Julia code is much more complex than a Wengert list, with constructs like control flow, data structures and function calls. To do this we'll have to handle each of these things in turn.","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"The first thing to realise is that Julia code is much closer to a Wengert list than it looks. Despite its rich syntax, the compiler works with a Wengert-like format. The analyses and optimisations that compilers already carry out also benefit from this easily-work-with structure.","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"f(x) = x / (1 + x^2)\n\n@code_typed f(1.0)","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"Code with control flow is pnly a little different. We add goto statements and a construct called the \"phi function\"; the result is called SSA form.","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"function pow(x, n)\n  r = 1\n  while n > 0\n    n -= 1\n    r *= x\n  end\n  return r\nend\n\npow(2, 3)\n\n@code_typed pow(2, 3)","category":"page"},{"location":"notebooks/reverse/","page":"Reverse","title":"Reverse","text":"The details of this format are not too important. SSA form is powerful but somewhat fiddly to work with in practice, so the aim of this notebook is to give a broad intuition for how we handle this.","category":"page"}]
}
